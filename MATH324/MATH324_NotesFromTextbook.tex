\documentclass[12 pt]{article}
\usepackage{hyperref, fancyhdr, setspace, enumerate, amsmath,
  lastpage, amssymb, tabularx}
\usepackage[margin=1 in]{geometry}
\allowdisplaybreaks
%\usepackage[dvipsnames]{xcolor}   %May be necessary if you want to color links
\hypersetup{
	%colorlinks=true, %set true if you want colored links
	linktoc=all,     %set to all if you want both sections and subsections linked
	linkcolor=black,  %choose some color if you want links to stand out
}
\usepackage{graphicx}
\graphicspath{{Images/}}

\usepackage{amsthm}
\newtheorem{thm}{Theorem}
\theoremstyle{definition}
\newtheorem{defn}{Definition}

% Independent symbol
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\author{Julian Lore}
\date{Last updated: \today}
\title{MATH 324: Statistics}
\pagestyle{fancy}
\lhead{MATH 324}
\chead{\leftmark}
\rhead{Julian Lore}
\cfoot{Page \thepage \ of \pageref{LastPage}}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
\begin{document}
	\onehalfspacing
	\maketitle
	Rough notes from Wackerly's Mathematical Statistics with
        Applications (7\textsuperscript{th} edition).
	\tableofcontents
        \section*{Information from Previous Chapters}
        Some formulas from probability:
        \begin{align*}
          Var(X) & = E[(X-\mu)^2] = E[X^2] - (E[X])^2
        \end{align*}
        Distributions:
        % Add the others later
        \begin{center}
          \begin{tabularx}{\textwidth}{c c c c c}
            \hline Distribution & Probability Function & Mean &
            Variance & Moment-Generating Function
            \\ \hline Binomial & $p(y)=\binom{n}{y}p^y(1-p)^{n-y}$ &
            $np$ & $np(1-p)$ & $[pe^t+(1-p)]^n$
          \end{tabularx}
        \end{center}
        \subsection*{6.7 Order Statistics}
        $Y_i$, with $i=1,\ldots,n$ independent continuous random
        variables. Denote ordered random variables by: $Y_{(1)} \leq
        Y_{(2)} \leq \ldots \leq Y_{(n)}$. We can then define:
        \begin{align*}
          Y_{(1)} & = \min{(Y_1, Y_2, \ldots, Y_n)}
          \\ Y_{(n)} & = \max{(Y_1, Y_2, \ldots, Y_n)}
        \end{align*}
        In order to find the pdf of $Y_{(1)}$ and $Y_{(n)}$, we use
        the method of distribution functions.
        \begin{align*}
          F_{Y_{(n)}}(y) & = P(Y_{(n)} \leq y) = P(Y_1\leq y, Y_2 \leq y, \ldots, Y_n \leq y)
          \\ &\stackrel{\text{ind}}{=}P(Y_1 \leq y)P(Y_2 \leq y)\ldots p(Y_n \leq y) = [F(y)]^n
               \intertext{Derive for the density:}
               g_{(n)}(y) & = n[F(y)]^{n-1}f(y)
        \end{align*}
        \noindent \rule{\textwidth}{0.5pt}
        \begin{align*}
          \\ F_{Y_{(1)}}(y) & = P(Y_{(1)} \leq 1) = 1 - P(Y_{(1)}>y) = 1 - 1 - P(Y_1>y, Y_2 > y, \ldots Y_n > y)
          \\ &\stackrel{\text{ind}}{=} 1-[P(Y_1 > y)P(Y_2>y)\ldots P(Y_n)>y] = 1-[1-F(y)]^n
               \intertext{Derive for the density:}
               g_{(1)}(y) & = n[1-F(y)]^{n-1}f(y)
        \end{align*}
        \section*{Other Useful Information}
        \begin{align*}
          E(\overline{Y}) & = E(Y)
        \end{align*}
        \setcounter{section}{7}
        \section{Estimation}
        \subsection{Introduction}
        Point of statistics is to use sample information to infer data
        about the population. Populations are characterized by numbers
        (\textit{parameters}) and we often want to estimate the value
        of parameter(s). Parameters include the proportion $p$,
        population mean $\mu$, variance $\sigma^2$ and standard
        deviation $\sigma$.
        \begin{defn}
          The parameter of interest in an experiment is called the
          \textit{target parameter}.
        \end{defn}
        \begin{defn}
          A \textit{point estimate} is a type of estimate where we use
          a single value/point to estimate a parameter. If we estimate
          a parameter by saying that it might fall between two
          numbers, this is an \textit{interval estimate}. We can use
          information from the sample to calculate these estimates,
          which are done using an estimator.
        \end{defn}
        \begin{defn}
          An \textit{estimator} is a rule, often expressed as a
          formula, that tells how to calculate the value of an
          estimate based on the measurements contained in a sample.
        \end{defn}
        \begin{defn}
          \textit{Sample mean:}
          $$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i$$ This is an
          example point estimator of $\mu$.
        \end{defn}
        There can be different estimators for the same population
        parameter. Some estimators are considered good and others are bad.
        \subsection{The Bias and Mean Square Error of Point
          Estimators}
        We cannot measure how good a point estimation procedure is
        with a single estimate, we need to observe the procedure many
        times. We create a frequency distribution to measure the
        goodness of a point estimator.
        \paragraph{Point Estimators} For a population parameter
        $\theta$, the estimator of $\theta$ is called $\hat{\theta}$.
        \begin{defn}
        Ideally, we'd want $E(\hat{\theta}) = \theta$. Point
        estimators that satisfy this are called
        \textit{unbiased}. Otherwise, they are called \textit{biased},
        where the \textit{bias} is given by
        $B(\hat{\theta})=E(\hat{\theta})-\theta$
      \end{defn}
      In addition, we'd also like the estimator $V(\hat{\theta})$ to
      be as small as possible, since a smaller variance guarantees a
      higher fraction of estimators to be ``close'' to $\theta$. If
      two estimators are unbiased and everything else is equal other
      than variance, we prefer the one with smaller variance.
      \begin{defn}
        Another way to characterize goodness of a point estimator is
        via its \textit{mean square error},
        $$MSE(\hat{\theta}) = E[(\hat{\theta}-\theta)^2]$$
        Which is the average of the square of the distance between the
        estimator and its target parameter. It can be shown that:
        $$MSE(\hat{\theta})=V(\hat{\theta})+[B(\hat{\theta})]^2$$
      \end{defn}
      \subsection{Some Common Unbiased Point Estimators}
      \begin{defn}
        $\sigma_{\hat{\theta}}^2$ denotes the variance of the sampling
        distribution of the estimator $\hat{\theta}$. The standard
        deviation $\sigma_{\hat{\theta}} =
        \sqrt{\sigma_{\hat{\theta}}^2}$ is called the \textit{standard
        error} of the estimator $\hat{\theta}$.
    \end{defn}
    \paragraph{Common Point Estimators} If random samples are
    independent we have:
    \\
    \begin{tabular}{|c c c c c|}
      \hline Target Parameter $\theta$ & Sample Size(s) & Point
      Estimator $\hat{\theta}$ & $E(\hat{\theta})$ & Standard Error
      $\sigma_{\hat{\theta}}$
      \\ \hline $\mu$ & $n$ & $\overline{Y}$ & $\mu$ &
      $\frac{\sigma}{\sqrt{n}}$
      \\ $p$ & $n$ & $\hat{p} = \frac{Y}{n}$ & $p$ &
      $\sqrt{\frac{pq}{n}}$
      \\ $\mu_1 - \mu_2$ & $n_1, n_2$ & $\overline{Y}_1 -
      \overline{Y}_2$ & $\mu_1 - \mu_2$ &
      $\sqrt{\frac{\sigma_1^2}{n_1}+ \frac{\sigma_2^2}{n_2}}$
      \\ $p_1 - p_2$ & $n_1,n_2$ & $\hat{p}_1 - \hat{p}_2$ & $p_1 -
      p_2$ & $\sqrt{\frac{p_1q_1}{n_1}+\frac{p_2q_2}{n_2}}$
      \\ \hline
    \end{tabular}\\
    Note, these are all \textbf{unbiased} estimators. Note that the expected
    values and standard errors for $\overline{Y}$ and $\overline{Y}_1
    - \overline{Y}_2$ are valid for any distribution of the population
    and all four estimators are approximately normal for large $n$
    (see CLT).
    \begin{defn}
      The sample variances,
      $$S^2 = \dfrac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}$$
      $$S'^2 = \dfrac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}$$
      are unbiased ($S^2$) and biased ($S'^2$) estimators for $\sigma^2$.
    \end{defn}
    \subsection{Evaluating the Goodness of a Point Estimator}
    \begin{defn}
      The \textit{error of estimation} $\varepsilon$ is the distance
      between an estimator and its target parameter:
      $$\varepsilon = |\hat{\theta} - \theta|$$
    \end{defn}
    Note that $\hat{\theta}$ is a random variable, so the error of
    estimation is a random quantity and thus we can make probability
    statements about it (rather than saying exactly how big it is for
    some estimate).

    The probability $P(\varepsilon < b)$ represents the fraction of
    times, in repeated sampling that $\hat{\theta}$ falls within $b$
    units of $\theta$.

    Sometimes we want to find a value of $b$ such that $P(\varepsilon
    < b) =$ some value, say $0.90$.

    If $\hat{\theta}$ is unbiased we can find a bound on $\varepsilon$
    by expressing $b$ as a multiple of the standard error. Take for
    example, $b=k \sigma_{\hat{\theta}}$, where $k \geq 1$, then by
    Tchebysheff's we know that $P(\varepsilon < k
    \sigma_{\hat{\theta}}) \geq 1 -
    \frac{1}{k^2}$. $b=2\sigma_{\hat{\theta}}$ is a good approximate
    bound on $\varepsilon$ in practice. By Tchebysheff we know that
    $P(\varepsilon < 2\sigma_{\hat{\theta}}) \geq .75$.
\end{document}